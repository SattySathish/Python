{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.functional import mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:/Users/Admin/Desktop/Coding/Tensorflow/Mobiledataset.csv\")\n",
    "dataset = dataset.drop([\"model\",\"base_color\",\"processor\"], axis=1)\n",
    "dataset[\"battery_capacity\"] = dataset[\"battery_capacity\"]/1000\n",
    "dataset[\"num_of_ratings\"] = dataset[\"num_of_ratings\"]/10000\n",
    "dataset[\"sales_price\"] = dataset[\"sales_price\"]/1000\n",
    "dataset[\"ROM\"] = dataset[\"ROM\"]/100\n",
    "label = dataset[\"sales\"]\n",
    "feature = dataset.drop(\"sales\",axis=1)\n",
    "feature = pd.get_dummies(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([430, 20]) torch.Size([430, 1])\n",
      "tensor([[-0.6617, -1.5233, -4.5237,  ..., -0.2930, -0.0969, 10.3199],\n",
      "        [-0.6617, -0.6059, -2.6272,  ...,  3.4128, -0.0969, -0.0969],\n",
      "        [-0.6617, -1.5233, -4.5237,  ..., -0.2930, -0.0969, 10.3199],\n",
      "        ...,\n",
      "        [ 0.3527,  0.3115,  0.3528,  ..., -0.2930, -0.0969, -0.0969],\n",
      "        [-1.1689, -1.0646, -0.4599,  ..., -0.2930, -0.0969, -0.0969],\n",
      "        [-1.1689, -1.0646, -1.5436,  ...,  3.4128, -0.0969, -0.0969]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9180\\433068428.py:4: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  features = torch.tensor(features, dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "features = np.array(feature)\n",
    "labels = np.array(label).reshape(-1,1)\n",
    "features = StandardScaler().fit_transform(features)\n",
    "features = torch.tensor(features, dtype = torch.float32)\n",
    "labels = torch.tensor(labels,dtype = torch.float32)\n",
    "print(features.shape, labels.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([430, 1])\n",
      "tensor([[0.3836],\n",
      "        [0.2815],\n",
      "        [0.3836],\n",
      "        [0.2425],\n",
      "        [0.2425],\n",
      "        [0.2400],\n",
      "        [0.2425],\n",
      "        [0.2400],\n",
      "        [0.2425],\n",
      "        [0.2425],\n",
      "        [0.2815],\n",
      "        [0.2425],\n",
      "        [0.2839],\n",
      "        [0.2522],\n",
      "        [0.2839],\n",
      "        [0.2435],\n",
      "        [0.2400],\n",
      "        [0.2400],\n",
      "        [0.2815],\n",
      "        [0.2522],\n",
      "        [0.2906],\n",
      "        [0.2435],\n",
      "        [0.2522],\n",
      "        [0.2400],\n",
      "        [0.2435],\n",
      "        [0.2435],\n",
      "        [0.2522],\n",
      "        [0.2425],\n",
      "        [0.2815],\n",
      "        [0.2435],\n",
      "        [0.2815],\n",
      "        [0.2839],\n",
      "        [0.3041],\n",
      "        [0.2425],\n",
      "        [0.2906],\n",
      "        [0.3836],\n",
      "        [0.2906],\n",
      "        [0.2906],\n",
      "        [0.2839],\n",
      "        [0.2906],\n",
      "        [0.2839],\n",
      "        [0.2655],\n",
      "        [0.3689],\n",
      "        [0.2504],\n",
      "        [0.2435],\n",
      "        [0.2435],\n",
      "        [0.2435],\n",
      "        [0.2425],\n",
      "        [0.2425],\n",
      "        [0.2425],\n",
      "        [0.2425],\n",
      "        [0.2435],\n",
      "        [0.2469],\n",
      "        [0.2425],\n",
      "        [0.2425],\n",
      "        [0.2522],\n",
      "        [0.2362],\n",
      "        [0.2303],\n",
      "        [0.2324],\n",
      "        [0.2293],\n",
      "        [0.2301],\n",
      "        [0.2293],\n",
      "        [0.2301],\n",
      "        [0.2324],\n",
      "        [0.2301],\n",
      "        [0.2303],\n",
      "        [0.2303],\n",
      "        [0.2378],\n",
      "        [0.2362],\n",
      "        [0.2293],\n",
      "        [0.2362],\n",
      "        [0.2269],\n",
      "        [0.2445],\n",
      "        [0.2269],\n",
      "        [0.2349],\n",
      "        [0.2349],\n",
      "        [0.2410],\n",
      "        [0.2445],\n",
      "        [0.2241],\n",
      "        [0.2269],\n",
      "        [0.2445],\n",
      "        [0.2241],\n",
      "        [0.2241],\n",
      "        [0.2349],\n",
      "        [0.2378],\n",
      "        [0.2410],\n",
      "        [0.2499],\n",
      "        [0.2318],\n",
      "        [0.2291],\n",
      "        [0.2291],\n",
      "        [0.2291],\n",
      "        [0.2346],\n",
      "        [0.2373],\n",
      "        [0.2373],\n",
      "        [0.2499],\n",
      "        [0.2445],\n",
      "        [0.2445],\n",
      "        [0.2014],\n",
      "        [0.2171],\n",
      "        [0.2378],\n",
      "        [0.2318],\n",
      "        [0.2318],\n",
      "        [0.2186],\n",
      "        [0.2236],\n",
      "        [0.2257],\n",
      "        [0.2257],\n",
      "        [0.2236],\n",
      "        [0.2186],\n",
      "        [0.2186],\n",
      "        [0.2346],\n",
      "        [0.1900],\n",
      "        [0.2194],\n",
      "        [0.2228],\n",
      "        [0.2228],\n",
      "        [0.2268],\n",
      "        [0.2268],\n",
      "        [0.2110],\n",
      "        [0.2110],\n",
      "        [0.2121],\n",
      "        [0.2121],\n",
      "        [0.2174],\n",
      "        [0.2174],\n",
      "        [0.2102],\n",
      "        [0.2089],\n",
      "        [0.2102],\n",
      "        [0.2089],\n",
      "        [0.2130],\n",
      "        [0.2261],\n",
      "        [0.2109],\n",
      "        [0.2109],\n",
      "        [0.2130],\n",
      "        [0.2261],\n",
      "        [0.2145],\n",
      "        [0.2145],\n",
      "        [0.2112],\n",
      "        [0.2169],\n",
      "        [0.2169],\n",
      "        [0.2174],\n",
      "        [0.2132],\n",
      "        [0.2132],\n",
      "        [0.2186],\n",
      "        [0.2072],\n",
      "        [0.2183],\n",
      "        [0.2183],\n",
      "        [0.2174],\n",
      "        [0.2187],\n",
      "        [0.2186],\n",
      "        [0.2185],\n",
      "        [0.2191],\n",
      "        [0.2187],\n",
      "        [0.2185],\n",
      "        [0.2187],\n",
      "        [0.2187],\n",
      "        [0.2190],\n",
      "        [0.2079],\n",
      "        [0.2125],\n",
      "        [0.2125],\n",
      "        [0.2044],\n",
      "        [0.2106],\n",
      "        [0.2106],\n",
      "        [0.2197],\n",
      "        [0.2190],\n",
      "        [0.2190],\n",
      "        [0.2202],\n",
      "        [0.2174],\n",
      "        [0.2174],\n",
      "        [0.2202],\n",
      "        [0.2190],\n",
      "        [0.2187],\n",
      "        [0.2187],\n",
      "        [0.2202],\n",
      "        [0.2191],\n",
      "        [0.2191],\n",
      "        [0.2180],\n",
      "        [0.2180],\n",
      "        [0.2191],\n",
      "        [0.2180],\n",
      "        [0.2188],\n",
      "        [0.2152],\n",
      "        [0.2186],\n",
      "        [0.2166],\n",
      "        [0.2257],\n",
      "        [0.2257],\n",
      "        [0.2087],\n",
      "        [0.2187],\n",
      "        [0.2187],\n",
      "        [0.2268],\n",
      "        [0.2268],\n",
      "        [0.1814],\n",
      "        [0.2013],\n",
      "        [0.2188],\n",
      "        [0.2158],\n",
      "        [0.2173],\n",
      "        [0.2157],\n",
      "        [0.2188],\n",
      "        [0.1907],\n",
      "        [0.2173],\n",
      "        [0.2084],\n",
      "        [0.2168],\n",
      "        [0.2261],\n",
      "        [0.1985],\n",
      "        [0.1985],\n",
      "        [0.2013],\n",
      "        [0.2173],\n",
      "        [0.2083],\n",
      "        [0.2084],\n",
      "        [0.2151],\n",
      "        [0.2151],\n",
      "        [0.2112],\n",
      "        [0.2074],\n",
      "        [0.2074],\n",
      "        [0.2187],\n",
      "        [0.2108],\n",
      "        [0.2191],\n",
      "        [0.2108],\n",
      "        [0.2072],\n",
      "        [0.2186],\n",
      "        [0.2187],\n",
      "        [0.2155],\n",
      "        [0.2155],\n",
      "        [0.2197],\n",
      "        [0.2044],\n",
      "        [0.2190],\n",
      "        [0.2268],\n",
      "        [0.2186],\n",
      "        [0.2257],\n",
      "        [0.2186],\n",
      "        [0.2152],\n",
      "        [0.2186],\n",
      "        [0.2187],\n",
      "        [0.2186],\n",
      "        [0.2166],\n",
      "        [0.2166],\n",
      "        [0.2079],\n",
      "        [0.2164],\n",
      "        [0.1814],\n",
      "        [0.1936],\n",
      "        [0.2087],\n",
      "        [0.2087],\n",
      "        [0.2102],\n",
      "        [0.2099],\n",
      "        [0.1936],\n",
      "        [0.2102],\n",
      "        [0.2157],\n",
      "        [0.2097],\n",
      "        [0.2097],\n",
      "        [0.2084],\n",
      "        [0.2104],\n",
      "        [0.2104],\n",
      "        [0.2087],\n",
      "        [0.2214],\n",
      "        [0.2214],\n",
      "        [0.2215],\n",
      "        [0.2215],\n",
      "        [0.2091],\n",
      "        [0.2091],\n",
      "        [0.2152],\n",
      "        [0.2227],\n",
      "        [0.2183],\n",
      "        [0.2150],\n",
      "        [0.2091],\n",
      "        [0.2150],\n",
      "        [0.2205],\n",
      "        [0.2259],\n",
      "        [0.2150],\n",
      "        [0.2414],\n",
      "        [0.2227],\n",
      "        [0.2225],\n",
      "        [0.2184],\n",
      "        [0.2192],\n",
      "        [0.2311],\n",
      "        [0.2298],\n",
      "        [0.2159],\n",
      "        [0.2243],\n",
      "        [0.2159],\n",
      "        [0.2184],\n",
      "        [0.2133],\n",
      "        [0.2376],\n",
      "        [0.2127],\n",
      "        [0.2204],\n",
      "        [0.2026],\n",
      "        [0.2195],\n",
      "        [0.2184],\n",
      "        [0.2181],\n",
      "        [0.2127],\n",
      "        [0.2161],\n",
      "        [0.2184],\n",
      "        [0.2127],\n",
      "        [0.2205],\n",
      "        [0.2184],\n",
      "        [0.2313],\n",
      "        [0.2263],\n",
      "        [0.2376],\n",
      "        [0.2149],\n",
      "        [0.2181],\n",
      "        [0.2214],\n",
      "        [0.2152],\n",
      "        [0.2244],\n",
      "        [0.2155],\n",
      "        [0.2158],\n",
      "        [0.2414],\n",
      "        [0.2307],\n",
      "        [0.2152],\n",
      "        [0.3715],\n",
      "        [0.2320],\n",
      "        [0.2163],\n",
      "        [0.2307],\n",
      "        [0.2225],\n",
      "        [0.2152],\n",
      "        [0.3266],\n",
      "        [0.3715],\n",
      "        [0.2163],\n",
      "        [0.2192],\n",
      "        [0.2178],\n",
      "        [0.2131],\n",
      "        [0.2147],\n",
      "        [0.2194],\n",
      "        [0.2161],\n",
      "        [0.2164],\n",
      "        [0.2456],\n",
      "        [0.2161],\n",
      "        [0.2049],\n",
      "        [0.2178],\n",
      "        [0.2185],\n",
      "        [0.2225],\n",
      "        [0.2181],\n",
      "        [0.2404],\n",
      "        [0.2185],\n",
      "        [0.3257],\n",
      "        [0.2147],\n",
      "        [0.2456],\n",
      "        [0.2194],\n",
      "        [0.2941],\n",
      "        [0.2541],\n",
      "        [0.2333],\n",
      "        [0.2404],\n",
      "        [0.2561],\n",
      "        [0.2282],\n",
      "        [0.2225],\n",
      "        [0.2163],\n",
      "        [0.2215],\n",
      "        [0.2178],\n",
      "        [0.2178],\n",
      "        [0.2178],\n",
      "        [0.2178],\n",
      "        [0.2263],\n",
      "        [0.2263],\n",
      "        [0.2274],\n",
      "        [0.2204],\n",
      "        [0.2194],\n",
      "        [0.2995],\n",
      "        [0.2215],\n",
      "        [0.2275],\n",
      "        [0.2243],\n",
      "        [0.2155],\n",
      "        [0.2545],\n",
      "        [0.2188],\n",
      "        [0.2247],\n",
      "        [0.2335],\n",
      "        [0.2215],\n",
      "        [0.2185],\n",
      "        [0.2158],\n",
      "        [0.2188],\n",
      "        [0.2411],\n",
      "        [0.2155],\n",
      "        [0.2188],\n",
      "        [0.2201],\n",
      "        [0.2494],\n",
      "        [0.2201],\n",
      "        [0.2509],\n",
      "        [0.2216],\n",
      "        [0.2423],\n",
      "        [0.2291],\n",
      "        [0.2327],\n",
      "        [0.2176],\n",
      "        [0.2176],\n",
      "        [0.2443],\n",
      "        [0.2225],\n",
      "        [0.2064],\n",
      "        [0.2864],\n",
      "        [0.1987],\n",
      "        [0.2741],\n",
      "        [0.2500],\n",
      "        [0.2463],\n",
      "        [0.2433],\n",
      "        [0.2356],\n",
      "        [0.2176],\n",
      "        [0.2182],\n",
      "        [0.2182],\n",
      "        [0.2463],\n",
      "        [0.2400],\n",
      "        [0.2291],\n",
      "        [0.2268],\n",
      "        [0.2268],\n",
      "        [0.2291],\n",
      "        [0.3967],\n",
      "        [0.2851],\n",
      "        [0.2424],\n",
      "        [0.2451],\n",
      "        [0.2196],\n",
      "        [0.2203],\n",
      "        [0.2724],\n",
      "        [0.2532],\n",
      "        [0.2064],\n",
      "        [0.2540],\n",
      "        [0.2438],\n",
      "        [0.2209],\n",
      "        [0.2749],\n",
      "        [0.2215],\n",
      "        [0.2476],\n",
      "        [0.2773],\n",
      "        [0.2881],\n",
      "        [0.2514],\n",
      "        [0.2471],\n",
      "        [0.2209],\n",
      "        [0.2711],\n",
      "        [0.2720],\n",
      "        [0.2212],\n",
      "        [0.3302],\n",
      "        [0.3228],\n",
      "        [0.2116],\n",
      "        [0.2388],\n",
      "        [0.2038],\n",
      "        [0.2438],\n",
      "        [0.3042],\n",
      "        [0.2916],\n",
      "        [0.2890],\n",
      "        [0.2182],\n",
      "        [0.2423],\n",
      "        [0.2960]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20,10),\n",
    "    torch.nn.LeakyReLU(), # activation functions\n",
    "    torch.nn.Linear(10,6),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(6,9),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(9,1)\n",
    ")\n",
    "\n",
    "preds = model(features)\n",
    "print(preds.shape)\n",
    "\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4273.2583, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse_loss(preds, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4360, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2020, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4040, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7969, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4138, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7770, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9818, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4754, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1245, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4652, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 15000\n",
    "for epoch in range(epochs):\n",
    "    preds = model(features)\n",
    "    loss = mse_loss(preds, labels)\n",
    "    \n",
    "    if (epoch+1)%(epochs//10) == 0:\n",
    "        print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50.4977], grad_fn=<AddBackward0>) tensor([50.0400])\n"
     ]
    }
   ],
   "source": [
    "x = 100\n",
    "preds = model(features[x])\n",
    "print(preds,labels[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
